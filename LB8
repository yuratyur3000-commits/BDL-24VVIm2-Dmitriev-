import torch
import re
from transformers import AutoModelForCausalLM, AutoTokenizer

# ============================================================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
# ============================================================================

print("üöÄ –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê –¢–ï–ö–°–¢–ê –° LLM")
print("="*60)

# –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct-1M"
print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {MODEL_NAME}...")

try:
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True
    )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ pad_token –¥–ª—è Qwen (–≤–∞–∂–Ω–æ!)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print(f"‚ö†Ô∏è  –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω pad_token = eos_token: {tokenizer.pad_token}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto",
        trust_remote_code=True
    )
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {model.device}")
    
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
    exit(1)

# ============================================================================
# 2. –ó–ê–ì–†–£–ó–ö–ê –¢–ï–ö–°–¢–ê –ò–ó –§–ê–ô–õ–ê
# ============================================================================

print(f"\n{'='*60}")
print("üìñ –ó–ê–ì–†–£–ó–ö–ê –¢–ï–ö–°–¢–ê –ò–ó –§–ê–ô–õ–ê")
print(f"{'='*60}")

FILE_TO_ANALYZE = "ENG_article.txt"

try:
    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
    encodings = ['utf-8', 'cp1251', 'latin-1']
    data = None
    
    for encoding in encodings:
        try:
            with open(FILE_TO_ANALYZE, 'r', encoding=encoding) as file:
                data = file.read()
            print(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π: {encoding}")
            break
        except UnicodeDecodeError:
            continue
    
    if data is None:
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª {FILE_TO_ANALYZE}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
    data = ' '.join(data.split())
    
    print(f"üìä –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(data)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"üìù –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {len(data.split())}")
    
    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    print(f"\nüëÅÔ∏è  –ü–†–ï–î–ü–†–û–°–ú–û–¢–† (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
    preview = data[:200] + "..." if len(data) > 200 else data
    print(preview)
    
except FileNotFoundError:
    print(f"‚ùå –§–∞–π–ª '{FILE_TO_ANALYZE}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ, —á—Ç–æ –∏ —Å–∫—Ä–∏–ø—Ç")
    exit(1)
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
    exit(1)


# ============================================================================
# 3. –ü–û–î–ì–û–¢–û–í–ö–ê –ü–†–û–ú–ü–¢–ê –î–õ–Ø LLM
# ============================================================================

print(f"\n{'='*60}")
print("üß† –ü–û–î–ì–û–¢–û–í–ö–ê –ü–†–û–ú–ü–¢–ê –î–õ–Ø LLM")
print(f"{'='*60}")

# –ë–µ—Ä–µ–º —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤)
text_chunk = data[:5000]

messages = [
    {
        "role": "system",
        "content": """–¢—ã ‚Äî —Ç–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞. –¢–≤–æ–∏ –ø—Ä–∞–≤–∏–ª–∞:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
2. –ù–ò–ö–û–ì–î–ê –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç, –≥–æ–≤–æ—Ä–∏ "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
4. –ö–æ–ø–∏—Ä—É–π –∏–º–µ–Ω–∞ –∏ –¥–∞—Ç—ã –¢–û–ß–ù–û –∫–∞–∫ –≤ —Ç–µ–∫—Å—Ç–µ
5. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (—Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π):
1. [–ì–æ–¥ –∏–ª–∏ "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"]
2. [–ò–º—è –∏–ª–∏ "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"] 
3. [–ò–º—è, –≥–æ–¥ –∏–ª–∏ "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"]"""
    },
    {
        "role": "user",
        "content": f"""–í–æ—Ç —Ç–µ–∫—Å—Ç. –ù–∞–π–¥–∏ —Ç–æ—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã:

–¢–ï–ö–°–¢:
{text_chunk}

–í–û–ü–†–û–°–´:
1. –ù–∞–π–¥–∏ –≤ —Ç–µ–∫—Å—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –≤ –∫–∞–∫–æ–º –≥–æ–¥—É –±—ã–ª–∞ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ –≤–∑—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ "vanishing gradient problem". –í –∫–∞–∫–æ–º –≥–æ–¥—É —ç—Ç–æ –±—ã–ª–æ?
2. –ù–∞–π–¥–∏ –≤ —Ç–µ–∫—Å—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∫—Ç–æ –≤ 1891 –≥–æ–¥—É —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª –º–µ—Ç–æ–¥ —É–Ω–∏—á—Ç–æ–∂–∞—é—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π–æ –∏–ª–∏ –æ 1891 –≥–æ–¥–µ. –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ 1891 –≥–æ–¥—É? –ö—Ç–æ —á—Ç–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª?
3. –ù–∞–π–¥–∏ –≤ —Ç–µ–∫—Å—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∫—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–∏–ª —Ü–µ–ø–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤ –∫–∞–∫–æ–º –≥–æ–¥—É, —Ç–æ –µ—Å—Ç—å "chain rule". –ö—Ç–æ –∏ –≤ –∫–∞–∫–æ–º –≥–æ–¥—É?

–û–¢–í–ï–¢ (—Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—ã—à–µ):
1."""
    }
]

try:
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    print(f"‚úÖ –ü—Ä–æ–º–ø—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω")
    print(f"üìè –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –ø—Ä–æ–º–ø—Ç–∞
    print(f"\nüëÅÔ∏è  –ù–ê–ß–ê–õ–û –ü–†–û–ú–ü–¢–ê:")
    print(text[:200] + "...")
    
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–æ–º–ø—Ç–∞: {e}")
    
    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞
    system_msg = messages[0]['content']
    user_msg = messages[1]['content']
    text = f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{user_msg}<|im_end|>\n<|im_start|>assistant\n"
    print(f"‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞")

# ============================================================================
# 4. –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –ò –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–í–ï–¢–ê
# ============================================================================

print(f"\n{'='*60}")
print("‚ö° –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –ò –ì–ï–ù–ï–†–ê–¶–ò–Ø")
print(f"{'='*60}")

try:
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    model_inputs = tokenizer(
        [text],
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=2048
    ).to(model.device)
    
    input_length = model_inputs['input_ids'].shape[1]
    print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {input_length} —Ç–æ–∫–µ–Ω–æ–≤")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª—å—é...")
    
    with torch.no_grad():
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=200,
            temperature=0.1,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1
        )
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç (–±–µ–∑ –ø—Ä–æ–º–ø—Ç–∞)
    input_ids_length = model_inputs['input_ids'].shape[1]
    response_ids = generated_ids[0][input_ids_length:]
    response = tokenizer.decode(response_ids, skip_special_tokens=True)
    
    print(f"‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
    
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
    response = "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"

# ============================================================================
# 5. –í–´–í–û–î –ò –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
# ============================================================================

print(f"\n{'='*60}")
print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê")
print(f"{'='*60}")

print("\nü§ñ –û–¢–í–ï–¢ LLM –ú–û–î–ï–õ–ò:")
print(response)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
try:
    with open('analysis_results.txt', 'w', encoding='utf-8') as f:
        f.write(f"–ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–ê: {FILE_TO_ANALYZE}\n")
        f.write("="*50 + "\n\n")
        f.write("–û–¢–í–ï–¢ LLM –ú–û–î–ï–õ–ò:\n")
        f.write(response + "\n\n")
          
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'analysis_results.txt'")
    
except Exception as e:
    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {e}")

print(f"\n{'='*60}")
print("üèÅ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
print(f"{'='*60}")
